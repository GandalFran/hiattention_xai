# KG-HiAttention

**Synergizing AI-based Knowledge Graphs and Deep Learning for Explainable Software Vulnerability Analysis**

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.1+](https://img.shields.io/badge/pytorch-2.1+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A neuro-symbolic framework for software vulnerability analysis that combines:

- **Semantic Encoding**: Pre-trained CodeT5 transformer for token-level code understanding
- **Structural Encoding**: Graph Attention Networks (GAT) over a lightweight CPG-based knowledge graph (CFG/DFG)
- **Expert Knowledge**: Static analysis features for vulnerability patterns
- **Neuro-Symbolic Fusion**: Multi-modal integration for vulnerability prediction with explainability
- **Graph-grounded XAI**: Developer-facing explanations via attention attribution and SHAP on program graphs

## ğŸ¯ Key Features

- **AI-based Knowledge Graphs**: CPG-inspired lightweight program graphs with typed CFG/DFG relations
- **Neuro-Symbolic Integration**: Combines neural representations (CodeT5) with symbolic structure (GAT over program graphs)
- **Multi-modal Fusion**: Semantic embeddings + structural graph context + expert static features
- **Explainability**: Graph-grounded explanations with faithfulness and stability proxies
- **Real-world Evaluation**: Tested on BigVul dataset (C/C++ vulnerabilities from Linux Kernel, Chrome, FFmpeg)
- **HPC Ready**: Optimized for single NVIDIA H100 80GB GPU

## ğŸ“Š Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  INPUT: Source Code Function                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                 â”‚
        â–¼                                 â–¼
   Token Sequence                 Lightweight CPG (KG)
   (CodeT5 tokenizer)            (CFG/DFG relations)
        â”‚                                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      KG-HiAttention Model          â”‚
        â”‚                                    â”‚
        â”‚  Level 1: Input Representation    â”‚
        â”‚  Level 2: Semantic (CodeT5)       â”‚
        â”‚  Level 3: Structural (GAT)        â”‚
        â”‚  Level 4: Expert Knowledge        â”‚
        â”‚  Level 5: Neuro-Symbolic Fusion   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      XAI Module                   â”‚
        â”‚  - Graph Attention Attribution    â”‚
        â”‚  - SHAP-style Explanations        â”‚
        â”‚  - Faithfulness/Stability Proxies â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
      Vulnerability Score + Explanation
```

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/Gandalfran/hiattention_xai.git
cd hiattention_xai

# Create environment
conda create -n kg_hiattention python=3.11 -y
conda activate kg_hiattention

# Install dependencies
pip install -r requirements.txt
```

### Dataset Preparation

```bash
# Download BigVul dataset
bash scripts/download_datasets.sh

# Preprocess and build program graphs
python scripts/preprocess_data.py --dataset bigvul --output_dir datasets/processed
```

### Training

```bash
# Train KG-HiAttention model
python scripts/full_train_paper.py --config config/training_config.yaml --gpu 0

# Quick training for testing
python scripts/quick_train.py --epochs 5
```

### Generate Explanations

```bash
# Generate graph-grounded explanations
python scripts/visualize_graph_xai.py --checkpoint checkpoints/best.pt --output_dir results/explanations
```

## ğŸ“ Project Structure

```
hiattention_xai/
â”œâ”€â”€ config/                  # Training configurations
â”œâ”€â”€ data/                    # CPG/KG building, preprocessing
â”‚   â”œâ”€â”€ code_parser.py      # C/C++ code parsing
â”‚   â”œâ”€â”€ graph_builder.py    # Lightweight CPG construction
â”‚   â””â”€â”€ preprocessor.py     # Dataset preprocessing
â”œâ”€â”€ models/                  # Neuro-symbolic models
â”‚   â”œâ”€â”€ semantic_encoder.py # CodeT5 encoding
â”‚   â”œâ”€â”€ graph_encoder.py    # GAT for program graphs
â”‚   â”œâ”€â”€ expert_encoder.py   # Static feature projection
â”‚   â””â”€â”€ kg_hiattention.py   # Complete fusion model
â”œâ”€â”€ explainability/          # XAI components
â”‚   â”œâ”€â”€ attribution.py      # Graph-based attribution
â”‚   â”œâ”€â”€ shap_explainer.py   # SHAP integration
â”‚   â””â”€â”€ faithfulness.py     # Faithfulness/stability metrics
â”œâ”€â”€ training/                # Training infrastructure
â”‚   â”œâ”€â”€ trainer.py
â”‚   â””â”€â”€ metrics.py
â”œâ”€â”€ evaluation/              # Evaluation framework
â”‚   â””â”€â”€ evaluator.py
â””â”€â”€ utils/                   # Utilities

scripts/                     # Experiment scripts
â”œâ”€â”€ download_datasets.sh     # Dataset acquisition
â”œâ”€â”€ preprocess_data.py       # Data preprocessing
â”œâ”€â”€ full_train_paper.py      # Main training script
â”œâ”€â”€ quick_train.py           # Quick training for testing
â””â”€â”€ visualize_graph_xai.py   # XAI visualization
```

## ğŸ–¥ï¸ HPC Usage

### Setup Environment

```bash
# Setup conda environment on HPC
bash scripts/setup_hpc_env.sh
```

### Download BigVul Dataset

```bash
# Download and extract BigVul dataset
bash scripts/download_datasets.sh ./datasets
```

### Run Training on HPC

```bash
# Run training with SLURM
sbatch scripts/slurm_full_paper.sh

# Or run directly
bash scripts/run_training.sh
```

## ğŸ“ˆ Results on BigVul Dataset

Performance comparison on the BigVul test set (C/C++ vulnerabilities):

| Model Type | Method | AUC-ROC | Recall | Explainability |
|------------|--------|---------|--------|----------------|
| Traditional ML | Hybrid Ensemble | 0.7785 | 0.78 | Feature Importance |
| Deep Learning | CodeT5-Base | 0.7372 | 0.71 | Attention Weights |
| Deep Learning | KG-XAI (Single Fusion) | 0.7601 | 0.75 | Multi-Modal |
| Deep Learning | **KG-HiAttention (Ensemble)** | **0.7859** | **0.79** | **Multi-Modal** |

**Key Insights:**
- **KG-HiAttention (Ensemble) achieves AUC-ROC of 0.7859**, surpassing the strong Hybrid Ensemble baseline (0.7785)
- The neuro-symbolic fusion of semantic (CodeT5) and structural (CPG) features, when stabilized via ensemble learning and focal loss, provides a superior decision boundary
- The incremental improvement from single fusion (0.7601) to ensemble (0.7859) highlights the importance of mitigating variance in small, imbalanced datasets
- Graph-grounded explanations are complemented with faithfulness (deletion/insertion AUC: 0.847/0.823) and stability (mean consistency: 0.893) proxies

## ğŸ”¬ Reproducing Paper Results

To reproduce the results from the paper:

```bash
# 1. Preprocess BigVul dataset
python scripts/preprocess_data.py --dataset bigvul

# 2. Train baselines
python scripts/train_baseline.py --model codet5 --output_dir results/codet5
python scripts/train_baseline.py --model hybrid --output_dir results/hybrid

# 3. Train KG-HiAttention
python scripts/full_train_paper.py --output_dir results/kg_hiattention

# 4. Evaluate explainability
python scripts/visualize_graph_xai.py --checkpoint results/kg_hiattention/best.pt
```

## ğŸ“œ License

MIT License - see [LICENSE](LICENSE) for details.
